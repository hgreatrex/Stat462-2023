---
title: "Tutorial 5: Loading Data"
editor: visual
---

```{r, include=FALSE}

# PACKAGES-----------------------------------------------
# Tutorial packages
library(vembedr)
library(skimr)
library(yarrr)
library(RColorBrewer)
library(GGally) 
library(tidyverse)
library(plotly)
library(readxl)
library(rvest)
library(biscale)
library(tidycensus)
library(cowplot)
library(units)

```
# Reading in and loading data

## In-built datasets

There are many datasets built into R, and even more that come with packages.  To load them you simply use the `data` command.  Typing data() will bring up a load of the possible datasets.  

For example, this loads the iris dataset:

```{r}
data("iris")

# From the dplyr package
glimpse(iris)
```

If you want to specify data from a specific package, we can also tell R that:

```{r}
data("pirates", package = "yarrr")
mean(pirates$parrots)
```


All the datasets in R have a help file by using the help menu or putting a ? in front of its name.  DO THIS IN THE CONSOLE NOT A CODE CHUNK.

```{r,eval=FALSE}
?pirates
```


<br>

## Loading data from Excel files

R can easily read in Microsoft Excel spreadsheets using the `readxl` package:


1. **Make sure the readxl package is loaded.**<br>E.g. is `library(readxl)` in your library code chunk?<br>Have you run the code chunk?

2. **Place your excel file in your project folder**.<br>E.g. here I placed *Data_frostday.xlsx* into my project folder. MAKE SURE YOU **OPEN** R-STUDIO USING YOUR LAB PROJECT!! If you are not sure what I mean see [Projects: How do I know if I am running one?](https://psu-spatial.github.io/stat462-2022/T1_R_Basics.html#212_How_do_I_know_it_has_worked) and [returning to your project](https://psu-spatial.github.io/stat462-2022/T1_R_Basics.html#213_Returning_to_your_lab_project)
  
3. **Make a new code chunk and add the read_excel() command e.g.**<br>
   ```{r,eval=FALSE}
   frost <- read_excel("Data_frostday.xlsx")
   ```
   Here the command is `read_excel()`, you are applying this to "frostdays.xlsx" (e.g. reading in an excel file with that name), then assigning the result to a variable called frost. Because you are using your project, R knows to look inside your project folder to find the file.

If this works, there should be no errors and nothing prints on the screen when you run the code chunk.  

When I ran it, in the environment tab, frost appeared with a description as a table with 76 rows (observations/obs), and 7 columns (variables).  In R, this type of table/spreadsheet is called a `data.frame`.

```{r}
# Read in the frost.xlsx file in my project folder and assign it to a variable called frost
library(readxl)
frost    <- read_excel("Data_frostdata.xlsx")
names(frost)
```

Or you can put the full file path in the read_excel command

```{r}
# Read in the frost.xlsx file in my project folder and assign it to a variable called frost
library(readxl)
frost    <- read_excel("/Users/hlg5155/Documents/GitHub/Teaching/Stat462-2023/Data_frostdata.xlsx")
names(frost)
```

<br>

### Troubleshooting

**It says it can't find the file:**
 - **Are you running the right project? e.g. does it say Lab 3 at the top of the screen?**
 - Did you put the file into your Lab folder?
 - Did you spell it right and include the full .xslx extension?
 - Did you use quote marks?
 
**It says read_excel doesn't exist**
 - Did you install the readxl package?
 - Did you load the readxl package? Go click the code chunk with the library command again!
 - Did you spell the command right? (case sensitive)
 - Did you use () afterwards so R understands that it's a command?

<br>

**Using the wizard:** Sometimes you just can't get it working.  In those cases, try the import wizard:<br>

 - Go to the file menu at the very top of the screen. Click import dataset, then From Excel. Use the wizard to find your file and get it looking correct. It will show you the code you need in the code preview.
 - Because we want to include this file in the markdown, rather than pressing OK, copy the code preview text and put it in your code chunk. DO NOT PUT THE VIEW LINE IN THERE, or every time you run it will open a new tab with the data.

<br>

## Reading in csv Files {#Tut6b_csv}

```{r, include=FALSE}
ozone   <- read.csv("Data_Ozone.csv")
```

.csv files are comma separated text files, you can read them into microsoft excel.  In R, you don't need any special package to read in a csv file 

1. Place the csv file into your project folder 

2. Use the `read_csv()` command to read it into R. Assign it to a variable or it will just print onto the screen 

3. Run the code chunk, then click on the variable name in the Environment quadrant to check that it read in correctly (especially make sure that column names have read in correctly) 

For example, for to read in a csv file on ozone and summarise:

```{r}
# Read in the some data on ozone
ozone    <- read.csv("Data_Ozone.csv")

# Check the column names, or click on its name in the Environment quadrant
summary(ozone)

```

<br>

### Troubleshooting

**It says it can't find the file:**
 - Are you running the right project? e.g. does it say Lab 2 at the top of the screen?
 - Did you put the file into your Lab 2 folder?
 - Did you spell it right and include the full .csv extension?
 - Did you use quote marks?


<br>

**Using the wizard:** Sometimes you just can't get it working.  In those cases, try the import wizard:<br>

 - Go to the file menu at the very top of the screen. Click import dataset, then From Excel. Use the wizard to find your file and get it looking correct. It will show you the code you need in the code preview.
 - Because we want to include this file in the markdown, rather than pressing OK, copy the code preview text and put it in your code chunk. DO NOT PUT THE VIEW LINE IN THERE, or every time you run it will open a new tab with the data.

## Reading in txt Files {#Tut6c_csv}

Same as above but you use the read.txt command.  You get a lot of options here, from telling R if it has headers/column names to changing the 'delimiter'.  See the help file and http://www.sthda.com/english/wiki/reading-data-from-txt-csv-files-r-base-functions for more.





# Summarising data

Here I will show a few examples for the houses dataset we were using in lectures

```{r,warning=FALSE,message=FALSE,echo=FALSE}
# Conduct a Shapiro-Wilk test for normality on the Price of houses
data("HousesNY", package = "Stat2Data")
```

## Looking at the data itself {#Tut8a1_basics}


To have a look at the data there are many options. You can:

 - click on its name in the environment tab
 - Type its name into the console or into a code chunk (e.g. for our table, type `piratedataset` into the console or a code chunk)
 - Run the command `View(variable_name)` (View is a command from the tidyverse package).<br> This will open the data in a new tab.
 - Run the command `head(variable_name)` to see the first 6 lines or so (good for quick checks)
 - Run the command `glimpse(variable_name)` to get a nice summary.
 - Run the command `names(variable_name)` to get the column names.
 - 
 
<br>
 
For example
 
```{r}
# Note, there are sometimes more columns to the right, use the arrow to see
head(HousesNY)
```

To see what the column names are, you can use the `names(dataset)` command

```{r}
names(HousesNY)
```

Or the glimpse command:

```{r}
glimpse(HousesNY)
```

To see how many columns and rows there are, you can use the `nrow()` and `ncol()` commands

```{r}
nrow(HousesNY)
ncol(HousesNY)
```

<br>
<br>

## Summary statistics

To look at the summaries there are a load of options. Choose your favourites:

 - `summary(dataset)`
 - `skim(dataset)` in the skimr package
 - `summarize(dataset)` in the papeR package. This looks pretty powerful, I'm just learning it

None are better or worse than others - simply choose what works for you in the moment.

```{r}
summary(HousesNY)
```

```{r}
library(skimr) # you would need to install this
skim(HousesNY)
```

```{r}
library(pillar) # you would need to install this
glimpse(HousesNY)
```

or 

```{r}
str(HousesNY)
```


To see what the column names are, you can use the names(dataset) command

```{r}
names(HousesNY)

```

To print the first few rows

```{r}
head(HousesNY)
```

To find the number of rows and columns

```{r}
nrow(HousesNY)

ncol(HousesNY)

#or both dimensions
dim(HousesNY)
```

Or you can do things manually, using the $ symbol to choose a column.  All of this is for the price column

```{r}
mean(HousesNY$Price)
median(HousesNY$Price)
mode(HousesNY$Price)
sd(HousesNY$Price)
var(HousesNY$Price)
IQR(HousesNY$Price)
range(HousesNY$Price)
```
### Missing: What if they say NA?

 
There are missing values in  some datasets - and by default, R will set the answer to statistics to also be missing. 
```{r}
example <- c(1,4,5,2,3,NA,2,4)
mean(example)
```


To ignore them in a given command, try adding ,na.rm=TRUE to the command e.g.
```{r}
mean(example, na.rm=TRUE)
```

To simply remove all rows with missing data, try the `na.omit()` command e g.

```{r}
test <- data.frame(A=c(1,3,4),B=c(NA,3,1))
test
```
```{r}
test2 <- na.omit(test)
test2
```

## Making tables

Sometimes we want to see how many rows there are in different categories.  The easiest way to do this is using the table command.  For example, in our New York data, we can see how many houses there are with each number of beds using 

```{r}
table(HousesNY$Beds)
```

So there are 19 rows in our dataset where the Beds column says 4 (AKA 19 houses in our sample with 4 beds).  Or we can look at a 2 dimensional table

```{r}
table(HousesNY$Beds, HousesNY$Baths)
```
So there are 10 houses with 4 beds and 2 baths


To make these look more professional there are a number of packages you can install and use. For example, ztable will take the output of table and format it in a pretty way.  This will look TERRIBLE when you run R as it's making html code. But when you press knit it will look beautiful

```{r, results='asis'}
# don't include the install line in your code, run it in the console
# install.package("ztable")

library(ztable)
library(magrittr)
options(ztable.type="html")

mytable <- table(HousesNY$Beds, HousesNY$Baths)

my_ztable =ztable(mytable) 
print(my_ztable,caption="Table 1. Basic Table")
```


## Correlation

To find the correlation between two variables, you can simply use the cor function e.g.

```{r}
cor(HousesNY$Price,HousesNY$Beds)
```

To see the correlation between ALL columns we can make a "correlation matrix"


### Covariance/correlation matrix {-}

Looking at correlations is a quick (but often misleading) way to assess what is happening.  Essentially we can look at the correlation between each column of data.


```{r,message=FALSE,warning=FALSE}

# Choose column names - let's say I don't care about location
colnames(HousesNY)

# Create plot - note I have message=TRUE and warning=TRUE turned on at the top of my code chunk
ggpairs(HousesNY[,c("Price","Beds" ,"Baths","Size" , "Lot"   )])

```

You can simply look at the correlations of any NUMERIC columns using the corrplot code.

```{r}
library(corrplot)
house.numeric.columns <- HousesNY[ , sapply(HousesNY,is.numeric)]

corrplot(cor(house.numeric.columns),method="ellipse",type="lower")
```

There are LOADS of other ways to run correlation plots here: https://www.r-graph-gallery.com/correlogram.html
Feel free to choose a favourite.

Importantly, remember back to this website - https://www.tylervigen.com/spurious-correlations.  Just because another variable is correlated with our response does not mean it HAS to be in the model.  It simply means that you might want to consider whether there is a reason for that correlation.






<br>
<br>

# Filtering and selecting data


```{r,include=FALSE,echo=FALSE}
# invisible data read
library(tidyverse)
library(sp)
library(sf)
library(readxl)
library(skimr)
library(tmap)
library(USAboundaries)
library(viridis)
```


## Selecting a specific column 

Here I am using the NYHouses data as an example.   Sometimes we want to deal with only one specific column in our spreadsheet/dataframe, for example applying the mean/standard deviation/inter-quartile range command to say just the Price column.

To do this, we use the $ symbol. For example, here I'm simply selecting the data in the elevation column only and saving it to a new variable called elevationdata.

```{r, eval=FALSE}
data("HousesNY")
price <- HousesNY$Price

price
```

Try it yourself.  You should have seen that as you typed the $, it gave you all the available column names to choose from.

This means we can now easily summarise specific columns. For example: 

 - `summary(HousesNY)` will create a summary of the whole spreadsheet, 
 - `summary(HousesNY$Price)` will only summarise the Price column.  
 - `mean(HousesNY$Price)` will take the mean of the Price column in the HousesNY dataframe.  
 
<br>





## Table command: counts per group {#Tut7b_table}

Sometimes we want to count the occurrences of some category in our dataset.  For example, if you look at the HousesNY, it might be interesting to know how many Houses had each number of bedrooms

To do this, we use the table command: 

```{r, eval=FALSE}
table(HousesNY$Beds)
```

or to see the number with each combination of bedrooms and bathrooms:

```{r, eval=FALSE}
table(HousesNY$Beds, HousesNY$Baths)
```

For more, this tutorial is excellent: https://www.cyclismo.org/tutorial/R/tables.html.

<br>


## "Group_by" command: statistics per group

What if we want to do more than just count the number of rows?  

Well, we can use the `group_by()` and `summarise()` commands and save our answers to a new variable.  

Here we are making use of the pipe symbol, %>%,  which takes the answer from group_by and sends it directly to the summarise command.

Here is some data on frost dates at weather stations.

```{r}

frost    <- readxl::read_excel("Data_frostdata.xlsx")
head(frost)
```

To summarise results by the type of weather station:

```{r}
frost.summary.type <- group_by(frost, by=Type_Fake) %>%
                          summarise(mean(Latitude),
                                    max(Latitude),
                                    min(Dist_to_Coast))
frost.summary.type
```

Here, my code is:

 - Splitting up the frost data by the Type_Fake column<br>(e.g. one group for City, one for Airport and one for Agricultural Research) 
 - For the data rows in *each group*, calculating the mean latitude, the maximum latitude and the minimum distance to the coast 
 - Saving the result to a new variable called frost.summary.type.
 - Printing the results on the screen e.g. the furthest North/maximum latitude of rows tagged Agricultural_Research_Station is 36.32 degrees.
 

## Filtering rows and columns 

Sometimes, we do not want to analyse at the entire data.frame.  Instead, we would like to only look at one (or more) columns or rows. 

There are several ways we can select data. 

 - To choose a specific column, we can use the `$` symbol to select its name (as described above)

 - If you know which number rows or columns you want, you can use **square brackets** to numerically select data.
 
Essentially our data follows the format: tablename[ROWS,COLUMNS]


```{r}
# This will select the data in the 5th row and 7th column
frost[5,7]

# This will select the 2nd row and ALL the columns 
frost[2,]

# This will select the 3rd column and ALL the rows
frost[,3]
# similar to using its name
frost$Type_Fake

# We can combine our commands, this will print the 13th row of the Longitude column 
# (no comma as we're only looking at one column)
frost$Longitude[13]

# The : symbol lets you choose a sequence of numbers e.g. 1:5 is 1 2 3 4 5
# So this prints out rows 11 to 15 and all the columns
frost[11:15,]

# The "c" command allows you to enter whatever numbers you like.  
# So this will print out rows 4,3,7 and the "Elevation" and "Dist_to_Coast" columns
frost[c(4,3,7), c("Elevation","Dist_to_Coast")]
```


You can also add questions and commands inside the square brackets.  For example here is the weather station with the lowest elevation.  You can see my command chose BOTH rows where elevation = 10.

```{r}

# which row has the lowest elevation
# note the double == (more below)
row_number <- which(frost$Elevation == min(frost$Elevation))

# choose that row
loweststtation <- frost[row_number ,  ]
loweststtation
```

```{r}
seaside <- frost[which(frost$Dist_to_Coast < 10) ,  ]
seaside
```


## which command

The which command essentially says "which numbers" meet a certain threshold

e,g,

```{r}
a <- 100:110
which(a > 107)
```

Or which rows:

```{r}
outlier_rows <- which(frost$Dist_to_Coast < 1.5)
```



### Deleting rows


Or if you know the row number you can use the minus - sign to remove.  Or just filter below.


```{r}
# remove row 6 and and overwrite
frost <- frost[-6 ,]

# remove columns 4 and 2 and save result to newdata and overwrite
newdata <- frost[, - c(2,4) ]

```





### The dplyr filter command (tidyverse)

Filtering means selecting rows/observations based on their values.  To filter in R, use the command `filter()` from the dplyr package. I tend to write it as `dplyr:filter()` to force it to be correct.


Here we can apply the filter command to choose specific rows that meet certain criteria

```{r, results="hide"}
filter(frost, State == "FL")
```

The double equal operator `==` means equal to.  The command is telling R to keep the rows in *frost* where the *State* column equals "FL". 

If you want a few categories, choose the %in% operator, using the `c()` command to stick together the categories you want.  For example, here are states in Florida and Virginia.

```{r, results="hide"}
filter(frost, State %in% c("FL","VA"))
```

We can also explicitly exclude cases and keep everything else by using the not equal operator `!=`.  The following code *excludes* airport stations.

```{r, results="hide"}
filter(frost, Type_Fake != "Airport")
```

What about filtering if a row has a value greater than a specified value?  For example, Stations with an elevation greater than 500 feet?

```{r, results="hide"}
filter(frost, Elevation > 500)
```

Or less-than-or-equal-to 200 feet.

```{r, results="hide"}

# or save the result to a new variable
lowland_stations <- filter(frost, Elevation < 200)
summary(lowland_stations)
```

<br>

In addition to comparison operators, filtering may also utilize logical operators that make multiple selections.  There are three basic logical operators: `&` (and), `|` (or), and `!` (not).  We can keep Stations with an *Elevation* greater than 300 **and** *State* in Alabama `&`.

```{r, results="hide"}
filter(frost, Elevation > 300 & State == "AL")
```

Use `|` to keep Stations with a *Type_Fake* of "Airport" **or** a last spring frost date after April (~ day 90 of the year).

```{r, results="hide"}
filter(frost, Type_Fake == "Airport" | Avg_DOY_SpringFrost > 90 )

```



### The dplyr arrange command (tidyverse)
\

We use the `arrange()` function to sort a data frame by one or more variables. You might want to do this to get a sense of which cases have the highest or lowest values in your data set or sort counties by their name. For example, let's sort in ascending order by elevation.

```{r}
arrange(frost, Latitude)
```

By default, `arrange()` sorts in ascending order. We can sort by a variable in descending order by using the `desc()` function on the variable we want to sort by. For example, to sort the dataframe by *Avg_DOY_SpringFrost* in descending order we use

```{r}
arrange(frost, desc(Avg_DOY_SpringFrost))
```

<br>
<br>


#  Making plots


## Where to get plot code and examples

There are three places I visit constantly:

- https://r-charts.com/distribution/ 
- https://www.r-graph-gallery.com/ 
- https://flowingdata.com/ 

For plots, we have *many* choices.  We can use what is built into R, or.. use the ggplot system where you add on layers to your plot using the + symbol, or use specialist packages such as ggstatplot or beeswarm.

If you are new to data visualisation, read these two articles

 - https://flowingdata.com/2014/10/23/moving-past-default-charts/
 - https://flowingdata.com/2012/05/15/how-to-visualize-and-compare-distributions/


## How to decide what to do

Plots are designed to do two things, allow you to see something in the data that you couldn't see in the numbers, plus communicate output in a compelling way.  Going beyond the basics or knowing the limitations of a plot will help you do this, so in these examples I have provided a range of complexity.

So far, I have focused on plots that slow you to visualise the distribution of your response variable.  You do not need to use them all!  Choose ones that work for you and your data.

1. Boxplots
2. Histograms
3. Violin plots (half boxplot half histogram)
3. Beeswarm


## Boxplots

Boxplots have been around over 40 years!  See their history and evolution here: http://vita.had.co.nz/papers/boxplots.pdf
  


In terms of your reports, you need to think of 3 things:
 - Why you are making the plot (quick look vs publication worthy final graphic)
 - What aspects of the data do you want to highlight (lots of data, comparing groups, weird distributions..)
 - What are your final requirements and personal style (colorblind friendly, you're drawn to a certain type of plot..)
 
So for boxplots.. they are especially good at allowing you to compare different groups of things or to look for multiple groups in a single response variable.  Here is a beautiful example made by Marcus Beckman on dissertation lengths (description here: https://beckmw.wordpress.com/2014/07/15/average-dissertation-and-thesis-length-take-two/ and code here: https://github.com/fawda123/diss_proc )

If there are only one or two variables, I often jump to the violin or histogram plots as they show more detail.



So.. how to make these yourselves.  You have a range of options!

### Basics (single boxplot)

Here is the most basic boxplot you can make.  I often start with this for my own use when exploring the data, then later decide which plots to "make pretty".

```{r}
boxplot(HousesNY$Price)
```

We can make better boxplots in base R (e.g. using no special packages/libraries).   See this tutorial for all the details: https://www.datamentor.io/r-programming/box-plot/  which goes through exactly what each line means.

```{r}

# one big command on separate lines
boxplot(HousesNY$Price,
        main = "House prices of Canton NY sample",
        xlab = "Price (Thousand USD)",
        col = "light blue",
        border = "dark blue",
        horizontal = TRUE,
        notch = TRUE)
```


There are specific plotting packages, the most famous being ggplot2 (there are data camp courses on it).  The absolute basics.  Here x is blank because we just want to look at the price column alone.

```{r}
library(ggplot2)

ggplot(HousesNY, aes(x ="", y = Price)) +    ## this loads the data
   geom_boxplot()                            ## and we choose a boxplot
```


Note for now, think of the %>% symbol and + symbol also as "one command on multiple lines..".  They allow you to build up layers of the plot. Data camp has more on this.

But with these we can easily do more sophisticated things.  For example, here's how to see the underlying data, which allows us to see something of the background distribution

https://r-charts.com/distribution/box-plot-jitter-ggplot2/

```{r}
# Basic box plot
ggplot(HousesNY, aes(x = "", y = Price)) + 
  geom_boxplot() +
  geom_jitter()
```


### Comparing groups

The basic code to see a boxplot split by group, in this case the price per number of beds:

```{r}
boxplot(HousesNY$Price ~ HousesNY$Beds)
```

The advantage of this is that you can be sure that you really did plot your columns of choice (e.g. you didn't mistakenly label anything).  Note, if you use a comma, rather than the "~" symbol, you will make one for each column - which is normally not useful!

```{r}
boxplot(HousesNY$Price,  HousesNY$Beds)
```
<br>


In GGplot comparing different groups:

```{r}
# Libraries
library(tidyverse)
library(hrbrthemes)
library(viridis)

# tell R that the beds column is categorical
HousesNY$Beds <- factor(HousesNY$Beds,
                     levels=c(min(HousesNY$Beds):max(HousesNY$Beds)))

# Plot
  ggplot(HousesNY, aes(x=Beds, y=Price)) +
    geom_boxplot() 

```

Or getting more complex 

```{r}
# Libraries
library(tidyverse)
library(hrbrthemes)
library(viridis)

# tell R that the beds column is categorical
# I already did this in the table section
#HousesNY$Beds <- as.factor(HousesNY$Beds)

# Plot
HousesNY %>%
  ggplot( aes(x=Beds, y=Price, fill=Beds) )+
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.5, alpha=0.8) +
    ggtitle("") +
    xlab("Beds")

```

or dotplots..

```{r}
ggplot(HousesNY,  aes(x=Beds, y=Price, fill=Beds)) +
  geom_boxplot() +
  geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5,binwidth=7)
```

There are MANY more options, plus code here:
https://www.r-graph-gallery.com/boxplot.html


and a delightful tutorial here: https://www.r-bloggers.com/2021/11/how-to-make-stunning-boxplots-in-r-a-complete-guide-with-ggplot2/


### Sophisticated 

Finally, we *can* get super fancy in base R - it's often a good way to learn how to code. I like this example because it shows many different aspects/useful commands in R programming. http://www.opiniomics.org/beautiful-boxplots-in-base-r/

```{r, tidy=FALSE}
library(RColorBrewer)

# create colours and colour matrix (for points)
m     <- as.matrix(HousesNY$Price)

col_main   <- colorRampPalette(brewer.pal(12, "Set3"), alpha=TRUE)(ncol(m))
col_transp <- colorspace::adjust_transparency(col_main, alpha = .3)

colsm   <-matrix(rep(col_main, each=nrow(m)), ncol=ncol(m))
colsm_tr <-matrix(rep(col_transp, each=nrow(m)), ncol=ncol(m))


# create some random data for jitter
r <-  (matrix(runif(nrow(m)*ncol(m)), nrow=nrow(m), ncol=ncol(m)) / 2) - 0.25

# get the greys (stolen from https://github.com/zonination/perceptions/blob/master/percept.R)
palette <- brewer.pal("Greys", n=9)
color.background = palette[2]
color.grid.major = palette[5]

# set graphical area
par(bty="n", bg=palette[2], mar=c(5,8,3,1))

# plot initial boxplot
boxplot(m~col(m), horizontal=TRUE, outline=FALSE, lty=1, 
        staplewex=0, boxwex=0.8, boxlwd=1, medlwd=1, 
        col=colsm_tr, xaxt="n", yaxt="n",xlab="",ylab="")

# plot gridlines
for (i in pretty(m,10)) {
	lines(c(i,i), c(0,20), col=palette[4])
}

# plot points
points(m, col(m)+r, col=colsm, pch=16)

# overlay boxplot
boxplot(m~col(m), horizontal=TRUE, outline=FALSE, lty=1, 
        staplewex=0, boxwex=0.8, boxlwd=1, medlwd=1, col=colsm_tr, 
        add=TRUE, xaxt="n", yaxt="n")

# add axes and title
axis(side=1, at=pretty(m,10), col.axis=palette[7], 
     cex.axis=0.8, lty=0, tick=NA, line=-1)
axis(side=1, at=50, labels="Price (Thousand USD)", 
     lty=0, tick=NA, col.axis=palette[7])
axis(side=2, at=1, col.axis=palette[7], cex.axis=0.8, 
     lty=0, tick=NA, labels="Sample 1", las=2)
axis(side=2, at=17/2, labels="Phrase", col.axis=palette[7], 
     lty=0, tick=NA, las=3, line=6)
title("House Prices in Canton NY")
```

Or if you wish to do the rainbow many group boxplot at the beginning, the code is here : https://github.com/fawda123/diss_proc/blob/master/diss_plot.R


<br>
<br>




## Violin plots

Violin plots combine the simplicity of a boxplot with a sense of the underlying distribution.  This is useful when you want a sense of both the symmetry of the data and the underlying distribution.  Highly recommended!  For a single variable, consider a box-plot-with-histogram (see below).

There are MANY on R graph gallery with code:
https://www.r-graph-gallery.com/violin.html


For example, for our data:

```{r}
# fill=name allow to automatically dedicate a color for each group
ggplot(HousesNY, aes(x=Beds, y=Price, fill=Beds)) + 
   geom_violin()
```


There's also a *beautiful* package  called `ggstatsplot` which allows a lot of detail (https://indrajeetpatil.github.io/ggstatsplot/)

For example, I love the plot below because it shows how much data in each group.

```{r}
# you might need to first install this.
library(ggstatsplot)

# i'm changing the middle mean point to be dark blue

ggbetweenstats(data = HousesNY,x = Beds,y = Price, 
               centrality.point.args=list(color = "darkblue"))
```

Or we can customise it even more using this tutorial to get results like this (https://www.r-graph-gallery.com/web-violinplot-with-ggstatsplot.html)


<br>
<br>


## Histograms

Especially just looking at a single response variable, it's useful to look immediately at the distribution itself.  Histograms are great for this, although you must be careful that the bin size doesn't impact your perception of results.  Adding in a boxplot is often useful

Here is the absolute basic histogram

```{r}
hist(HousesNY$Price)
```

Or changing the bin size

```{r}
hist(HousesNY$Price,br=40)
```

In GGPlot 2, it's also easy

```{r}

ggplot(data=HousesNY, aes(x=Price)) + 
  geom_histogram(bins=20) 

```


Often, a boxplot AND a histogram is useful as it allows you to see a sense of the data shape and its underlying symmetry.  For example, in base R

```{r,tidy=FALSE}
# Layout to split the screen
graphics::layout(matrix(c(1,2),2,1, byrow=TRUE),  
       height = c(2,7))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, .5, 2.1))

data_to_plot <- HousesNY$Price

rangeplot <- pretty(data_to_plot,10)

boxplot(data_to_plot,col = "light blue",
        border = "dark blue",xaxt="n",frame=FALSE,xlim=c(0.75,1.25),
        horizontal = TRUE,notch = TRUE,ylim=c(min(rangeplot),max(rangeplot)))

par(mar=c(3, 3.1, .5, 2.1))
hist(data_to_plot , breaks=20 , 
     col=grey(0.3) , border=F , 
     tcl=-.25,mgp=c(1.75,.5,0),
     main="" , xlab="Price of houses in Canton NY", 
     xlim=c(min(rangeplot),max(rangeplot)))
box();grid();
hist(data_to_plot , breaks=20 , add=TRUE,
     col=grey(0.3) , border=F , axis=FALSE,
     xlim=c(min(rangeplot),max(rangeplot)))
```



And the same with ggplot2:

```{r}
library(ggExtra)

p <- ggplot(data=HousesNY, aes(x=Price)) + 
  geom_point(aes(y = 0.01), alpha = 0) +
  geom_histogram(bins=20) +
  geom_density(na.rm=T)

ggMarginal(p, type="boxplot", margins = "x")

```



I also love the ggstatplot version 

```{r}

library(ggstatsplot)
## plot
gghistostats(
  data       = HousesNY, ## dataframe from which variable is to be taken
  x          = Price, ## numeric variable whose distribution is of interest
  title      = "Price of sampled houses in Canton NY", ## title for the plot
  caption    = "Source: Zillow",
  type = "parametric",
  xlab = NULL,subtitle=FALSE,
  ggthemes::theme_tufte(),
  binwidth   = 8) ## binwidth value (experiment)
```

Or their version that includes a lot of associated statistics.  You can turn many of these on and off

```{r}
library(ggstatsplot)

## plot
gghistostats(
  data       = HousesNY, ## dataframe from which variable is to be taken
  x          = Price, ## numeric variable whose distribution is of interest
  title      = "Price of sampled houses in Canton NY", ## title for the plot
  caption    = "Source: Zillow",
  type = "parametric",
  xlab = NULL,
  ggthemes::theme_tufte(),
  binwidth   = 8) ## binwidth value (experiment)
```
### Adding a density function

Sometimes seeing a smoothed line helps draw the eye to distributions

```{r}
hist(HousesNY$Price, prob = TRUE,
     main = "Canton Prices with density curve")
lines(density(HousesNY$Price), col = 4, lwd = 2)
box()
```



### Adding a distribution

Let's say you want to make plots similar to the ones in the lectures where there is your chosen distribution on top.

If you know the distribution, you can simply add it on top as a line

```{r}
mysample <- HousesNY$Price

plotmin <- mean(mysample) - sd(mysample)*3
plotmax <-  mean(mysample) + sd(mysample)*3

# Points for the normal equation line
NormCurve_x <- seq(plotmin,plotmax, length = 40)

# Normal curve calculation for each point
NormCurve_y <- dnorm(NormCurve_x, mean = mean(mysample), sd = sd(mysample))

# make sure this is density not raw frequency
hist(mysample , breaks=20 , freq=FALSE,
     col=grey(0.5) , border=F , 
     xlim=c(plotmin,plotmax),
     tcl=-.25,mgp=c(1.75,.5,0),
     main="" , xlab="Price of houses in Canton NY")
# add the normal curve (THIS NEEDS TO BE IN THE SAME CODE CHUNK)
lines(NormCurve_x, NormCurve_y, col = 2, lwd = 2)
box()

```

We could plot any old curve this way, it doesn't have to be "fit" to our data.  For example here is a random gamma function

```{r}
mysample <- HousesNY$Price

# Points for the normal equation line
GammaCurve_x <- seq(plotmin,plotmax, length = 60)
GammaCurve_y <- dgamma(GammaCurve_x,shape = 2)

# make sure this is density not raw frequency
hist(mysample , breaks=20 , freq=FALSE,
     col=grey(0.5) , border=F , 
     xlim=c(plotmin,plotmax),
     tcl=-.25,mgp=c(1.75,.5,0),
     main="" , xlab="Price of houses in Canton NY")
# add the normal curve (THIS NEEDS TO BE IN THE SAME CODE CHUNK)
lines(GammaCurve_x, GammaCurve_y, col = 2, lwd = 2)
box()

```




### Comparing groups

Or you can easily compare two datasets, tutorial for this plot here:
https://www.r-graph-gallery.com/histogram_several_group.html


<br>
<br>

## Ridgeline plots

These are another way of looking at histograms for different groups.  They work especially when your grouping data is ORDINAL (has some inherent order).  So bedrooms would be a good example

Two great pages here:
 - https://www.data-to-viz.com/graph/ridgeline.html
 - https://r-charts.com/distribution/ggridges/


We can use histograms or smoothed density lines
https://www.data-to-viz.com/graph/ridgeline.html

```{r}

library(ggridges)
library(ggplot2)

HousesNY %>%
  ggplot( aes(y=Beds, x=Price,  fill=Beds)) +
    geom_density_ridges(alpha=0.6, stat="binline") +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    theme_ipsum() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
    xlab("") +
    ylab("Number of Bedrooms")

```

But the

All of these are from
https://r-charts.com/distribution/ggridges/


```{r,message=FALSE}
library(ggridges)
library(ggplot2)

ggplot(HousesNY, aes(x = Price, y = Beds, fill = stat(x))) +
  geom_density_ridges_gradient() +
  scale_fill_viridis_c(name = "Depth", option = "C") +
  coord_cartesian(clip = "off") + # To avoid cut off
  theme_minimal()
```


We can also make the colours more meaningful, for example adding quantiles to show the median and interquartile range

```{r}
ggplot(HousesNY, aes(x = Price, y = Beds, fill = stat(quantile))) +
  stat_density_ridges(quantile_lines = FALSE,
                      calc_ecdf = TRUE,
                      geom = "density_ridges_gradient") +
  scale_fill_brewer(name = "")
```

or highlighting tails

```{r}
ggplot(HousesNY, aes(x = Price, y = Beds, fill = stat(quantile))) +
  stat_density_ridges(quantile_lines = TRUE,
                      calc_ecdf = TRUE,
                      geom = "density_ridges_gradient",
                      quantiles = c(0.05, 0.95)) +
  scale_fill_manual(name = "Proportion", 
                    values = c("#E2FFF2", "white", "#B0E0E6"),
                    labels = c("(0, 5%]", "(5%, 95%]", "(95%, 1]"))
```



## Beeswarm plots

These are cool.  As described here:

https://www.rhoworld.com/i-swarm-you-swarm-we-all-swarm-for-beeswarm-plots-0/#:~:text=What%20is%20a%20beeswarm%20plot%3F&text=A%20beeswarm%20plot%20improves%20upon,bees%20buzzing%20about%20their%20hive.

"But what is a beeswarm plot? ... A beeswarm plot improves upon the random jittering approach to move data points the minimum distance away from one another to avoid overlays. The result is a plot where you can see each distinct data point, like so: It looks a bit like a friendly swarm of bees buzzing about their hive."

It's often used for professional visualisation, see here for many examples: https://flowingdata.com/charttype/beeswarm


Especially for the first, you can see the distribution clearly, also with the amount of data.  With the second, you can see the mitigating impact of a second variable.

To make easy ones you can install a new packages "beeswarm"


```{r}
library("beeswarm")

beeswarm(HousesNY$Price,
         vertical = FALSE, method = "hex")
```

This is a little boring for my 58 data points!  (although perhaps it does show that 58 points is barely a big enough sample to know an underlying model..)


<br>
<br>

## Scatterplots


Here is the absolute basic scatterplot

```{r}
# you can either do plot(x, y)
# OR (recommended), use the ~ to say plot(y~x) 
# e.g. y depends on x
HousesNY$Beds <- as.numeric(HousesNY$Beds)

plot(HousesNY$Price ~ HousesNY$Beds)
```

There are many things we can change, see the help file for the `par` command for more.

For example, here is an ugly plot showing as many as I can think!

```{r}

plot(HousesNY$Price ~ HousesNY$Beds,
     xlim=c(0,7), #xlimits
     ylim=c(40,220), #ylimits
     xlab=list("Beds",cex=.8,col="red",font=2), # play with x-label
     ylab=list("Price",cex=1.2,col="blue",font=3), # play with x-label
     main="Ugly feature plot",
     cex=1.2, #point size
     pch=16, # symbol shape (try plot(1:24,1:24,pch=1:24 to see them all))
     tcl=-.25, # smaller tick marks
     mgp=c(1.75,.5,0)) # move the x/y labels around

grid() #  add a grid

# lines means "add points on top"
lines(HousesNY$Price ~ HousesNY$Beds, 
     type="p", # p for points, "l" for lines, "o" for both, "h for bars
     xlim=c(0,7), #xlimits
     ylim=c(40,220), #ylimits
     col="yellow",
     cex=.5, #point size
     pch=4) # move the x/y labels around



```

To add a line, you can use the abline command IN THE SAME CODE CHUNK.  For example


```{r}

plot(HousesNY$Price ~ HousesNY$Beds,
     xlim=c(0,7), #xlimits
     ylim=c(40,220), #ylimits
     xlab=list("Beds",cex=.8,col="red",font=2), # play with x-label
     ylab=list("Price",cex=1.2,col="blue",font=3), # play with x-label
     main="", # no title
     cex=1.2, #point size
     pch=16, # symbol shape (try plot(1:24,1:24,pch=1:24 to see them all))
     tcl=-.25, # smaller tick marks
     mgp=c(1.75,.5,0)) # move the x/y labels around

# add vertical line at 3.5
abline(v=5.5,col="red")
# add horizontal line at the mean of price
abline(h=mean(HousesNY$Price)) 
# add line of best fit
abline(lm(HousesNY$Price ~ HousesNY$Beds),col="blue",lty="dotted",lwd=3) 


```




GGPlot also has basic and advanced options.  From the basics:

```{r}
library(ggplot2)
#
ggplot(HousesNY, aes(x=Beds, y=Price)) + 
    geom_point()

```

To more advanced:
```{r}
library(ggplot2)
library(hrbrthemes)

# use options!
ggplot(HousesNY, aes(x=Beds, y=Price)) + 
    geom_point(
        color="black",
        fill="#69b3a2",
        shape=22,
        alpha=0.5,
        size=6,
        stroke = 1
        ) +
    theme_ipsum()
```

Adding a line of best fit is also easy, but fits less easily with R's modelling commands:

```{r}
# Library
library(ggplot2)
library(hrbrthemes)

# Create dummy data
data <- data.frame(
  cond = rep(c("condition_1", "condition_2"), each=10), 
  my_x = 1:100 + rnorm(100,sd=9), 
  my_y = 1:100 + rnorm(100,sd=16) 
)

# Basic scatter plot.
p1 <- ggplot(data, aes(x=my_x, y=my_y)) + 
  geom_point( color="#69b3a2") +
  theme_ipsum()
 
# with linear trend
p2 <- ggplot(data, aes(x=my_x, y=my_y)) +
  geom_point() +
  geom_smooth(method=lm , color="red", se=FALSE) +
  theme_ipsum()

# linear trend + confidence interval
p3 <- ggplot(data, aes(x=my_x, y=my_y)) +
  geom_point() +
  geom_smooth(method=lm , color="red", fill="#69b3a2", se=TRUE) +
  theme_ipsum()

p1
p2
p3
```

Or using the plotly library to make your plots interactive (really useful, try zooming in or clicking on a few points)

```{r}
# create the plot, save it as "p" rather than print immediately
myplot <-   ggplot(HousesNY, aes(x=Beds, y=Price, label=Baths)) + 
            geom_point(alpha=.5) +
            theme_classic()
            
# and plot interactively
ggplotly(myplot)
```




It's also very easy to add in color to see another variable.  For example

```{r}
# create the plot, save it as "p" rather than print immediately
myplot <-   ggplot(HousesNY, aes(x=Beds, y=Price,color=Baths)) + 
            geom_point(alpha=.5) +
            theme_classic()+
            scale_color_gradient(low="blue", high="red")

# and plot interactively
ggplotly(myplot)
```

Many more interactive options in this tutorial: https://plotly.com/r/line-and-scatter/ 





#  Distributions and tests {#S.Tutorial.2D}

We have talked about several distributions and tests so far in the lab.  To see the help files for most of them, see `?Distributions`

## Normal distribution {#S.Tutorial.2D.1}

Remember as we discussed in lectures, we normally state that a variable is ~N(mean, VARIANCE).  But in these commands you need the standard deviation instead. (you can google how to get the sd from the variance if you have forgotten)

To see the help file for all these:
```{r,eval=FALSE}
?Normal
```

To generate a random sample from a normal distribution: 
```{r}
sample.normal <- rnorm(n=100,mean=4,sd=2)
```

To calculate a z score from your sample/population, you can use R as a calculator. 

To calculate the probability of greater/lesser than a value in a given normal distribution (e.g. you can use this as an interactive table)

```{r}
# probability of less than 1.7 in a normal distribution of N(4,2^2)
pnorm(1.7,mean=4,sd=2,lower.tail = TRUE)

# probability of greater than 1.8 in a normal distribution of N(4,2^2)
1 - pnorm(1,mean=4,sd=2,lower.tail = TRUE)
# or
pnorm(1,mean=4,sd=2,lower.tail = FALSE)
```

To calculate the value for a given probability

```{r}
# what value is less than 60% of the data?
qnorm(0.6,mean=4,sd=2,lower.tail = TRUE)

# what value is greater than 80% of the data?
qnorm(0.8,mean=4,sd=2,lower.tail = FALSE)
```


### Wilks Shapiro test for normality {#S.Tutorial.2D.3}

To test for normality: 

First, have a look at the histogram!  Here is the code for the Shapiro-Wilk test.

```{r}
shapiro.test(HousesNY$Price)
```

You can also make a QQ-Norm plot

We discussed the basic qqnorm command last week: `qqplot(variable)`.  For example *qqplot(malepirates$age)* makes a qq-norm plot of the age column in the data.frame we created earlier on male pirates.  There is a nicer version inside the ggpubr package.

```{r, eval=FALSE}
library(ggpubr)
ggqqplot(HousesNY$Price,col="blue")
```

YOU CAN INTERPRET IT HERE: https://www.learnbyexample.org/r-quantile-quantile-qq-plot-base-graph/


## Student's t-distribution  {#S.Tutorial.2D.2}

What even is this?  See this nice resource: https://365datascience.com/tutorials/statistics-tutorials/students-t-distribution/


To see the help file for all these:

```{r,eval=FALSE}
?TDist
```

To calculate a t-statistic from your sample/population, you can use R as a calculator.  To calculate the probability of greater/lesser than a value in a given t-distribution (e.f. you can use this as an interactive t-table)

```{r}
# probability of seeing less than 1.7 in a  t-distribution 
# with 20 degrees of freedom
pt(1.55,df=20,lower.tail = TRUE)

```

To calculate the value for a given probability

```{r}
# what value is greater than 90% of the data in a t-distribution with df=25
qt(0.9,df=25,lower.tail = TRUE)
```

To conduct a full t-test on some data:

```{r}

# Conduct a two-sided t-test where we think that the data comes from a T-distribution with mean 100.
t.test(HousesNY$Price,mu=100,alternative="two.sided")
```

or see the detailed tutorial here: http://www.sthda.com/english/wiki/one-sample-t-test-in-r for one-sample

and here for comparing two samples: http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r



<br>
<br>

# Regression models

Now we will fit our first regression model.  

## "Standard" regression output {#S.Tutorial.3B.1}

The command to do this is `lm()` e.g. linear model.

```{r,eval=FALSE}
output <- lm(y_column ~ x_column,data=tablename)
output
```

NOTE, THE WEIRD ~ SYMBOL. This means "depends on" and it's how we tell R what the response variable is.  E.g. y depends on x, or y=mx+c.  

For example for the NYHouses data, it would be

```{r}
# response = Price, predictor = Lot size
Model1.lm <- lm(Price ~ Lot,data=HousesNY)
Model1.lm
```

So we are saying here that the equation is

Expected_Average_Price  =  -0.5749*Lot_Size  + 114.0911

E.g. the average expected price house with no Lot/Garden is 114.09

You can also directly get the code for the model equation by the equatiomatic package

```{r,eval=FALSE}
# YOU MIGHT NEED TO INSTALL THIS PACKAGE (SEE THE TUTORIAL)
library(equatiomatic)
extract_eq(Model1.lm,use_coefs=FALSE)
```

To make it print out directly, put "asis=TRUE" as a code chunk option e.g. this code


```{r, eqn, echo=FALSE,fig.cap="See the asis in the top, this prints the output directly when you knit"}
knitr::include_graphics('./Figures/Tut7_extract_eq.png')
```

Turns into this:


```{r,asis=TRUE}
library(equatiomatic)
extract_eq(Model1.lm,use_coefs=FALSE)
```

You can also look at the summary by looking at the summary command:

```{r}
summary(Model1.lm)
```

In both cases, we have an estimate of the intercept (0.6386) and of the gradient (-13.8103).  We will discuss the other values in later labs/lectures.

Now let's see how to add the regression line to our scatterplot.  We can do this using `abline(REGRESSION_VARIABLE)`, where regression_variable is the name of the variable you saved the output of lm to.  For example.

```{r}
plot(HousesNY$Price ~ HousesNY$Lot)
abline(lm(Price ~ Lot,data=HousesNY),col="blue",lwd=1) 

```

For more professional plots, see the scatterplots tutorial

## "Fancy" OLSRR regression output {#S.Tutorial.3B.2}

If you want a different way of seeing the same output, you can use the `ols_regress()` command inside the `olsrr` package.  

```{r}
library(olsrr)
Model1.lm.ols <- ols_regress(Price ~ Lot,data=HousesNY)
Model1.lm.ols
```

The ols_regress command produces beautiful output, but sometimes it doesn't work well with other commands.  So I tend to run a lm command at the same time to have both available.

Sometimes, this command can produce a weird error:

```{r, olsrr.error, echo=FALSE,fig.cap="This is probably because you loaded the moderndive package"}
knitr::include_graphics('./Figures/Tut7_OLSRR.png')
```

This is probably because you loaded the moderndive package. They do not play nicely together. Save your work, restart R and **do not run any line that says library(moderndive)!**.
